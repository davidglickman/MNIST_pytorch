{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEISjb0pvY3OuEE0bJOoTO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKVffeLgcNPq","executionInfo":{"status":"ok","timestamp":1731063915115,"user_tz":-120,"elapsed":136696,"user":{"displayName":"David Glickman","userId":"04192506760093579239"}},"outputId":"5e23a99b-6ea6-43b5-f63c-f25956adc372"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 16.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 501kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 4.53MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 2.51MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch [1/5], Step [100/938], Loss: 0.3948\n","Epoch [1/5], Step [200/938], Loss: 0.2443\n","Epoch [1/5], Step [300/938], Loss: 0.4219\n","Epoch [1/5], Step [400/938], Loss: 0.4439\n","Epoch [1/5], Step [500/938], Loss: 0.3812\n","Epoch [1/5], Step [600/938], Loss: 0.4354\n","Epoch [1/5], Step [700/938], Loss: 0.3376\n","Epoch [1/5], Step [800/938], Loss: 0.2022\n","Epoch [1/5], Step [900/938], Loss: 0.1785\n","Epoch [2/5], Step [100/938], Loss: 0.2662\n","Epoch [2/5], Step [200/938], Loss: 0.2153\n","Epoch [2/5], Step [300/938], Loss: 0.3241\n","Epoch [2/5], Step [400/938], Loss: 0.5033\n","Epoch [2/5], Step [500/938], Loss: 0.1386\n","Epoch [2/5], Step [600/938], Loss: 0.2114\n","Epoch [2/5], Step [700/938], Loss: 0.1653\n","Epoch [2/5], Step [800/938], Loss: 0.1399\n","Epoch [2/5], Step [900/938], Loss: 0.2521\n","Epoch [3/5], Step [100/938], Loss: 0.1212\n","Epoch [3/5], Step [200/938], Loss: 0.0788\n","Epoch [3/5], Step [300/938], Loss: 0.1682\n","Epoch [3/5], Step [400/938], Loss: 0.0871\n","Epoch [3/5], Step [500/938], Loss: 0.0900\n","Epoch [3/5], Step [600/938], Loss: 0.0965\n","Epoch [3/5], Step [700/938], Loss: 0.1186\n","Epoch [3/5], Step [800/938], Loss: 0.1394\n","Epoch [3/5], Step [900/938], Loss: 0.1453\n","Epoch [4/5], Step [100/938], Loss: 0.0964\n","Epoch [4/5], Step [200/938], Loss: 0.2441\n","Epoch [4/5], Step [300/938], Loss: 0.1234\n","Epoch [4/5], Step [400/938], Loss: 0.2460\n","Epoch [4/5], Step [500/938], Loss: 0.0835\n","Epoch [4/5], Step [600/938], Loss: 0.1588\n","Epoch [4/5], Step [700/938], Loss: 0.2074\n","Epoch [4/5], Step [800/938], Loss: 0.0991\n","Epoch [4/5], Step [900/938], Loss: 0.0385\n","Epoch [5/5], Step [100/938], Loss: 0.1307\n","Epoch [5/5], Step [200/938], Loss: 0.1144\n","Epoch [5/5], Step [300/938], Loss: 0.0579\n","Epoch [5/5], Step [400/938], Loss: 0.2320\n","Epoch [5/5], Step [500/938], Loss: 0.0972\n","Epoch [5/5], Step [600/938], Loss: 0.1344\n","Epoch [5/5], Step [700/938], Loss: 0.0775\n","Epoch [5/5], Step [800/938], Loss: 0.0164\n","Epoch [5/5], Step [900/938], Loss: 0.0569\n","Test Accuracy: 97.01%\n"]}],"source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# Hyperparameters\n","input_size = 784  # 28x28 images flattened\n","hidden_size = 128\n","num_classes = 10  # Digits 0-9\n","num_epochs = 5\n","batch_size = 64\n","learning_rate = 0.001\n","\n","# Define transformations for data preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n","])\n","\n","# Load the MNIST dataset\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","\n","# Data loaders for batch processing\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Define the neural network model\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        x = x.view(-1, input_size)  # Flatten the input\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = NeuralNet(input_size, hidden_size, num_classes)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()  # For multi-class classification\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n","\n","# Testing the model\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        outputs = model(images)\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"]}]}